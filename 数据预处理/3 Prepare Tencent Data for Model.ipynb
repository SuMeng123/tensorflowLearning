{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 串词抽取——数据准备\n",
    "0、提前将json数据中的title article 分词，将结果缓存到本地 <br>\n",
    "1、读取数据，将article与title合并 <br>\n",
    "2.1 去除最原始的脏数据，比如：文章只有一句“每日更新超多有趣漫画，喜欢请关注”、文章是空的、题目是空的 <br>\n",
    "2.2 去除标点、停用词  <br>\n",
    "2.3 根据长度范围，过滤comment article <br>\n",
    "3、balance数据 <br>\n",
    "4、对train dec数据集shuffle <br>\n",
    "5、build dict（基于pre-trian word2vec/基于训练数据集） <br>\n",
    "6、生成id化的训练集与测试集 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\Anaconda3\\envs\\tf\\lib\\site-packages\\tqdm\\autonotebook\\__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import collections\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.autonotebook import tqdm\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "import jieba\n",
    "import re\n",
    "\n",
    "filename_train = r'D:\\data\\comment_generation\\article_commenting_learning_comment_generation\\newdata.train.json'\n",
    "filename_dev = r'D:\\data\\comment_generation\\article_commenting_learning_comment_generation\\newdata.dev.json'\n",
    "filename_test = r'D:\\data\\comment_generation\\article_commenting_learning_comment_generation\\newdata.test.json'\n",
    "\n",
    "filename_cut_train = r'D:\\data\\comment_generation\\article_commenting_learning_comment_generation\\newdata.cut.train.json'\n",
    "filename_cut_dev = r'D:\\data\\comment_generation\\article_commenting_learning_comment_generation\\newdata.cut.dev.json'\n",
    "filename_cut_test = r'D:\\data\\comment_generation\\article_commenting_learning_comment_generation\\newdata.cut.test.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1、加载 train test dev 数据,把article_wb与title_wb合并为article_wb\n",
    "def load_data(filename):\n",
    "    data = []\n",
    "    with open(filename, encoding='utf-8') as fin:\n",
    "        for line in fin:\n",
    "            item = json.loads(line)\n",
    "            title = item['title_es'].strip()\n",
    "            article = item['body'].strip()\n",
    "            article_wb = item['article_wb'].strip()\n",
    "            title_wb = item['title_wb'].strip()\n",
    "            url = item['url']\n",
    "            uid = url.split('/')[-1]\n",
    "            for comment in item['comment']:\n",
    "                comment_content = ''.join(comment[0])\n",
    "                upvote = int(comment[1])\n",
    "                data.append({\n",
    "                    'article_id': uid,\n",
    "                    'title': title,\n",
    "                    'article': article,\n",
    "                    'comment': comment_content,\n",
    "                    'upvote': upvote,\n",
    "                    'article_wb':(article_wb+\" \"+title_wb).strip()\n",
    "                })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def split_comment(x):\n",
    "    return len(x.split(\" \"))\n",
    "\n",
    "\n",
    "def filter_article(df):\n",
    "    mask = ((df['article_wb'] == \"\") | (df['article_wb'] == 'null\\n') | (df['article_wb'] == 'null') | (df['title'] == 'null') | (df['title'].isnull()) | (df[\"article\"] == \"更多趣味内涵内容，敬请关注微信公众号：漫画精选集（jx2018mh）\" )| (df[\"article\"] == \"每日更新超多有趣漫画，喜欢请关注\") |(df[\"article\"] == \"文章已被原作者删除，目前无法查看。\")|\n",
    "            (df[\"article_wb\"].apply(split_comment)<6) | (df[\"article_wb\"].apply(split_comment)>300)|(df[\"comment\"].apply(split_comment)>30))\n",
    "    df = df[~mask]\n",
    "    return df\n",
    "\n",
    "def filter_comment(df):\n",
    "    mask = ((df['article'] == \"\") | (df['article'] == 'null\\n') | (df['article'] == 'null') | (df['title'] == 'null') | (df['title'].isnull()) | (df[\"article\"] == \"更多趣味内涵内容，敬请关注微信公众号：漫画精选集（jx2018mh）\" )| (df[\"article\"] == \"每日更新超多有趣漫画，喜欢请关注\") |(df[\"article\"] == \"文章已被原作者删除，目前无法查看。\")|\n",
    "            (df[\"comment\"].apply(split_comment)<3)|(df['comment'] == \"\")|(df['comment'] == 'null'))\n",
    "    df = df[~mask]\n",
    "    return df\n",
    "\n",
    "\n",
    "def sample_neg(grouped_df, count):\n",
    "    neg_samples = grouped_df[grouped_df['upvote'] < 10]\n",
    "    neg_sample_len_lte_100 = neg_samples[neg_samples['comment'].str.len() <= 100]\n",
    "    neg_sample_len_gt_100 = neg_samples[neg_samples['comment'].str.len() > 100]\n",
    "\n",
    "    if len(neg_sample_len_lte_100) >= count:\n",
    "        return neg_sample_len_lte_100.sample(n=count, random_state=1)\n",
    "    else:\n",
    "        lte_100_samples = neg_sample_len_lte_100\n",
    "        gt_100_count = min(len(neg_sample_len_gt_100), count - len(neg_sample_len_lte_100))\n",
    "        gt_100_samples = neg_sample_len_gt_100.sample(n=gt_100_count, random_state=1)\n",
    "        return pd.concat([lte_100_samples, gt_100_samples])\n",
    "\n",
    "\n",
    "def sample_random_neg(grouped, name, su_grouped_df, count):\n",
    "    names = []\n",
    "    samples_neg = []\n",
    "    count_name = count\n",
    "    while (len(names) == 0):\n",
    "        if len(grouped.groups.keys()) > count_name:\n",
    "            names = random.sample(grouped.groups.keys(), count_name)\n",
    "            if name in names:\n",
    "                names.remove(name)\n",
    "        else:\n",
    "            count_name = len(grouped.groups.keys()) - 1\n",
    "\n",
    "    article = su_grouped_df.iat[0, 0]\n",
    "    article_id = su_grouped_df.iat[0, 1]\n",
    "    title = su_grouped_df.iat[0, 3]\n",
    "    upvote = 1\n",
    "    for n in names:\n",
    "        temp = grouped.get_group(n).sample(n=1, random_state=1)\n",
    "        temp[\"article\"] = article\n",
    "        temp[\"article_id\"] = article_id\n",
    "        temp[\"title\"] = title\n",
    "        temp[\"upvote\"] = upvote\n",
    "        samples_neg.append(temp)\n",
    "    return pd.concat(samples_neg)\n",
    "\n",
    "def save(df, save_to):\n",
    "    df.to_csv(save_to, sep='\\t', encoding='utf-8')\n",
    "\n",
    "def load(filename):\n",
    "    return pd.read_csv(filename, sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  加载停用词，选取的是哈工大停用词的汉字部分\n",
    "stopwords = []\n",
    "def fetch_stopwords(file = \"./resources/stopwords.txt\"):\n",
    "    with open(file,\"r\",encoding=\"utf8\") as f:\n",
    "        textLines = f.readlines()\n",
    "        for line in textLines:\n",
    "            stopwords.append(line.strip())\n",
    "fetch_stopwords()\n",
    "\n",
    "\n",
    "# 去除标点和停用词（必须先分词，才能执行此方法）\n",
    "def remove_punctuation(line):\n",
    "    rule = re.compile(\"[^a-zA-Z0-9\\u4e00-\\u9fa5]\")\n",
    "    line = rule.sub('', line)\n",
    "    if line in stopwords:\n",
    "        return \"\"\n",
    "    return line\n",
    "\n",
    "# 对article tiltle 分词\n",
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self._cache = dict()\n",
    "\n",
    "    def cut(self, text):\n",
    "        if text in self._cache:\n",
    "            return self._cache[text]\n",
    "        wb = list(jieba.cut(text))\n",
    "        wb = ' '.join(wb)\n",
    "        self._cache[text] = wb\n",
    "        return wb\n",
    "\n",
    "# 对评论 去除标点、停用词\n",
    "class CleanPunStop:\n",
    "    def __init__(self):\n",
    "        self._cache = dict()\n",
    "\n",
    "    def clean(self, text):\n",
    "        if text in self._cache:\n",
    "            return self._cache[text]\n",
    "        comment = text.split(\" \")\n",
    "        comment = list(map(lambda x: remove_punctuation(x), comment))\n",
    "        while \"\" in comment:\n",
    "            comment.remove(\"\")\n",
    "        comment = ' '.join(comment)\n",
    "        self._cache[text] = comment\n",
    "        return comment\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "cleanPunStop = CleanPunStop()\n",
    "tqdm.pandas(desc='word break:')\n",
    "\n",
    "def text_to_ids(word_dict, text_wb):\n",
    "    words = text_wb.split()\n",
    "    ids = []\n",
    "    unk_id = word_dict['<unk>']\n",
    "    for word in words:\n",
    "        word_id = word_dict.get(word, unk_id)\n",
    "        ids.append(word_id)\n",
    "    return ids\n",
    "\n",
    "\n",
    "def transform_data(word_dict,df, save_to, inputs_length=200, outputs_length=30):\n",
    "    with open(save_to, 'w', encoding='utf-8') as fout:\n",
    "        for idx, row in tqdm(df.iterrows()):\n",
    "            article_wb = row['article_wb']\n",
    "            comment_wb = row['comment']\n",
    "            label = 1 if row['upvote'] >= 10 else 0\n",
    "#             title_ids = text_to_ids(word_dict, title_wb)\n",
    "            article_ids = text_to_ids(word_dict, article_wb)\n",
    "            comment_ids = text_to_ids(word_dict, comment_wb)\n",
    "            inputs = article_ids\n",
    "            inputs = inputs[0: inputs_length]\n",
    "            inputs = ' '.join(str(it) for it in inputs)\n",
    "            targets = comment_ids[0: outputs_length]\n",
    "            targets = ' '.join(str(it) for it in targets)\n",
    "            fout.write(f'{label}\\t{inputs}\\t{targets}\\n')\n",
    "            \n",
    "\n",
    "def cut_art_title(read_path, write_path):\n",
    "    from tqdm import tqdm\n",
    "    with open(read_path, \"r\", encoding=\"utf8\") as f2:\n",
    "        with open(write_path, \"w+\", encoding=\"utf8\") as f1:\n",
    "            textLines = f2.readlines()\n",
    "            print(\"读取原始文件\" + read_path)\n",
    "            for i, line in enumerate(tqdm(textLines)):\n",
    "                line = json.loads(line)\n",
    "                line['article_wb'] = \" \".join(list(jieba.cut(line['body'].strip())))\n",
    "                line['title_wb'] = \" \".join(list(jieba.cut(line['title_es'].strip())))\n",
    "                f1.write(json.dumps(line, sort_keys=True, separators=(',', ': '), ensure_ascii=False))\n",
    "                f1.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0、提前将json数据中的title article 分词，将结果缓存到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取原始文件D:\\data\\comment_generation\\article_commenting_learning_comment_generation\\newdata.train.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 191502/191502 [12:40<00:00, 251.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取原始文件D:\\data\\comment_generation\\article_commenting_learning_comment_generation\\newdata.dev.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:19<00:00, 254.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取原始文件D:\\data\\comment_generation\\article_commenting_learning_comment_generation\\newdata.test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1610/1610 [00:05<00:00, 309.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# （如果已经执行过，无需再执行）将分词后的结果缓存到本地，以后直接读取\n",
    "cut_art_title(filename_train,filename_cut_train)\n",
    "cut_art_title(filename_dev,filename_cut_dev)\n",
    "cut_art_title(filename_test,filename_cut_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、读取数据，将article与title合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始加载 train dev 数据,将article与title合并为aritcle_wb...\n"
     ]
    }
   ],
   "source": [
    "# 1、加载 train dev 数据，将aritle_wb 与 title_wb 合并为aritcle_wb\n",
    "print(\"开始加载 train dev 数据,将article与title合并为aritcle_wb...\")\n",
    "df_train = load_data(filename_cut_train)\n",
    "df_dev = load_data(filename_cut_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['article', 'article_id', 'article_wb', 'comment', 'title', 'upvote']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     5131167\n",
       "unique    4763025\n",
       "top           傻 逼\n",
       "freq         2869\n",
       "Name: comment, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"comment\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                             5131167\n",
       "unique                             168749\n",
       "top       文章 已 被 原作者 删除 ， 目前 无法 查看 。 null\n",
       "freq                               604000\n",
       "Name: article_wb, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"article_wb\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 去除最原始的脏数据，比如：文章只有一句“每日更新超多有趣漫画，喜欢请关注”、文章是空的、题目是空的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_original(df):\n",
    "    mask = ((df['article_wb'] == \"\") | (df['article_wb'] == 'null\\n') | (df['article_wb'] == 'null') | (df['article_wb'].isnull()) | (df[\"article\"] == \"更多趣味内涵内容，敬请关注微信公众号：漫画精选集（jx2018mh）\" )| (df[\"article\"] == \"每日更新超多有趣漫画，喜欢请关注\") |(df[\"article\"] == \"文章已被原作者删除，目前无法查看。\")\n",
    "           )\n",
    "    df = df[~mask]\n",
    "    return df\n",
    "\n",
    "clean1_df_train = filter_original(df_train)\n",
    "clean1_df_dev = filter_original(df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                                               4524725\n",
       "unique                                               168720\n",
       "top       击 上面 　 　 免费 订阅 ！ 这 两天 又 一个 新闻 将 韩国 人气 坏 了 ！ 特朗...\n",
       "freq                                                     84\n",
       "Name: article_wb, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean1_df_train[\"article_wb\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     4524725\n",
       "unique    4218247\n",
       "top           傻 逼\n",
       "freq         2477\n",
       "Name: comment, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean1_df_train[\"comment\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 去除标点、停用词 （耗时22分钟）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a1b51856fb9461594da720167767da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='word break:', max=4524725, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\Anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13bbcc68456f46d59961fb024bc1bb45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='word break:', max=4524725, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\Anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e82e978ccd48c9928641b554c97d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='word break:', max=118783, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\Anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360ddd715ea54705bc7d6e3b88e9db1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='word break:', max=118783, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\Anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# 去除comment article字段的标点、停用词\n",
    "clean1_df_train[\"comment\"] = clean1_df_train[\"comment\"].progress_apply(cleanPunStop.clean)\n",
    "clean1_df_train[\"article_wb\"] = clean1_df_train[\"article_wb\"].progress_apply(cleanPunStop.clean)\n",
    "\n",
    "clean1_df_dev[\"comment\"] = clean1_df_dev[\"comment\"].progress_apply(cleanPunStop.clean)\n",
    "clean1_df_dev[\"article_wb\"] = clean1_df_dev[\"article_wb\"].progress_apply(cleanPunStop.clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除一些由于去掉停用词而变空的comment 或者 article\n",
    "def filter_null(df):\n",
    "    mask = ((df['article_wb'] == \"\") |  (df['article_wb'] == 'null') | (df['article_wb'].isnull()) | \n",
    "           (df['comment'].isnull())|(df['comment'] == \"\")| (df['comment'] == 'null') )\n",
    "    df = df[~mask]\n",
    "    return df\n",
    "\n",
    "clean2_df_train = filter_null(clean1_df_train)\n",
    "clean2_df_dev = filter_null(clean1_df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     118117\n",
       "unique    112922\n",
       "top          傻 逼\n",
       "freq         101\n",
       "Name: comment, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean2_df_dev[\"comment\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                                               4499793\n",
       "unique                                               168668\n",
       "top       小米 6 还 未 发布 时 已 寄予厚望 雷军 饥饿 营销 制造 小米 6 一机 难求 景象...\n",
       "freq                                                     84\n",
       "Name: article_wb, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean2_df_train[\"article_wb\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "su_grouped_test = clean2_df_dev.groupby(['article_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4439"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(su_grouped_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 根据长度范围，过滤comment article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    118117.000000\n",
       "mean          9.819789\n",
       "std          14.467491\n",
       "min           1.000000\n",
       "25%           4.000000\n",
       "50%           6.000000\n",
       "75%          11.000000\n",
       "max        1177.000000\n",
       "Name: comment, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 先看一下长度的分布\n",
    "a_com_ci = clean2_df_dev['comment'].map(lambda x:x.split(\" \"))\n",
    "a_com_ci.str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_article_length(df):\n",
    "    mask = ((df[\"article_wb\"].apply(split_comment)<20) )\n",
    "    df = df[~mask]\n",
    "    return df \n",
    "\n",
    "def filter_comment_length(df):\n",
    "    mask = ((df[\"comment\"].apply(split_comment)<3))\n",
    "    df = df[~mask]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_train_article = filter_article_length(clean2_df_train)\n",
    "cleaned_dev_article = filter_article_length(clean2_df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4443663"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_train_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_train_article_comment = filter_comment_length(cleaned_train_article)\n",
    "cleaned_dev_article_comment = filter_comment_length(cleaned_dev_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3853273"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_train_article_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    101050.000000\n",
       "mean         11.014904\n",
       "std          14.606550\n",
       "min           3.000000\n",
       "25%           5.000000\n",
       "50%           7.000000\n",
       "75%          13.000000\n",
       "max        1177.000000\n",
       "Name: comment, dtype: float64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_com_ci = cleaned_dev_article_comment['comment'].map(lambda x:x.split(\" \"))\n",
    "a_com_ci.str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4396"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "su_grouped_test = cleaned_dev_article_comment.groupby(['article_id'])\n",
    "len(su_grouped_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、balance数据（>2耗时23分钟，>10耗时10分钟，数据共1710545条，有79320条title_id）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.710545e+06\n",
       "mean     5.106490e+00\n",
       "std      3.932079e+01\n",
       "min      0.000000e+00\n",
       "25%      0.000000e+00\n",
       "50%      0.000000e+00\n",
       "75%      3.000000e+00\n",
       "max      1.646300e+04\n",
       "Name: upvote, dtype: float64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df_article_comment['upvote'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始对数据集进行正负平衡...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84393100c737405c82d49581d9b44b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=166751), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正负均衡后数据集size： 811557\n"
     ]
    }
   ],
   "source": [
    "# 3(1) 对train数据集进行正负平衡（负例评论是来自其他文章下的评论）\n",
    "print(\"开始对数据集进行正负平衡...\")\n",
    "su_grouped = cleaned_train_article_comment.groupby(['article_id'])\n",
    "su_samples = []\n",
    "for su_name, su_grouped_df in tqdm(su_grouped):\n",
    "    su_grouped_pos_df = su_grouped_df[su_grouped_df['upvote'] >= 10]\n",
    "    su_pos_count = len(su_grouped_pos_df)\n",
    "    if su_pos_count == 0:\n",
    "        continue\n",
    "    su_grouped_neg_df = sample_random_neg(su_grouped, su_name, su_grouped_df, su_pos_count)\n",
    "    su_samples.append(su_grouped_pos_df)\n",
    "    su_samples.append(su_grouped_neg_df)\n",
    "\n",
    "sampled_df_train = pd.concat(su_samples)\n",
    "\n",
    "# 将正负均衡后的训练集数据保存到本地，以后直接从本地读取即可\n",
    "save(sampled_df_train, './5resources/train_balance.csv')\n",
    "print(\"正负均衡后数据集size：\",len(sampled_df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始对dev数据集进行正负平衡...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a28a69efa1142e39d93f75d5b3ef5af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4396), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正负均衡后数据集size： 18799\n"
     ]
    }
   ],
   "source": [
    "# 3(2) 对dev数据集进行正负平衡（负例评论是来自其他文章下的评论）\n",
    "print(\"开始对dev数据集进行正负平衡...\")\n",
    "su_grouped = cleaned_dev_article_comment.groupby(['article_id'])\n",
    "su_samples = []\n",
    "for su_name, su_grouped_df in tqdm(su_grouped):\n",
    "    su_grouped_pos_df = su_grouped_df[su_grouped_df['upvote'] >= 10]\n",
    "    su_pos_count = len(su_grouped_pos_df)\n",
    "    if su_pos_count == 0:\n",
    "        continue\n",
    "    su_grouped_neg_df = sample_random_neg(su_grouped, su_name, su_grouped_df, su_pos_count)\n",
    "    su_samples.append(su_grouped_pos_df)\n",
    "    su_samples.append(su_grouped_neg_df)\n",
    "\n",
    "sampled_df_dev = pd.concat(su_samples)\n",
    "\n",
    "# 将正负均衡后的训练集数据保存到本地，以后直接从本地读取即可\n",
    "save(sampled_df_dev, './5resources/dev_balance.csv')\n",
    "print(\"正负均衡后数据集size：\",len(sampled_df_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、对train dec数据集shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = sampled_df_train.sample(frac=1)\n",
    "dev= sampled_df_dev.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['article', 'article_id', 'article_wb', 'comment', 'title', 'upvote']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5、build dict（基于train数据集）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c601fe16cfd44b619a337e243ec3511c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词汇表的size： 740469\n"
     ]
    }
   ],
   "source": [
    "counter = collections.Counter()\n",
    "for article_wb, comment_wb in tqdm(\n",
    "        zip(train['article_wb'], train['comment'])):\n",
    "    for wb in [article_wb, comment_wb]:\n",
    "        words = wb.split()\n",
    "        counter.update(words)\n",
    "\n",
    "counter = counter.most_common()\n",
    "word_dict = dict()\n",
    "word_dict['<pad>'] = 0\n",
    "word_dict['<unk>'] = 1\n",
    "for word, count in counter:\n",
    "#     if count < 20:\n",
    "#         break\n",
    "    word_dict[word] = len(word_dict)\n",
    "\n",
    "with open('./5resources/'+str(len(word_dict))+'_word_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(word_dict, f)\n",
    "print(\"词汇表的size：\", len(word_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5、build dict（基于pre trian word2vec）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "base_path = \"D:\\\\data\\\\搜狗新闻word2vec\\\\\"\n",
    "embeddingFile = \"corpus.vector\"\n",
    "embeddingFile = base_path + embeddingFile\n",
    "\n",
    "def load_word_dict_ms(filename):\n",
    "    \"\"\"\n",
    "    加载词向量文件\n",
    "\n",
    "    :param filename: 文件名\n",
    "    :return: embeddings列表和它对应的索引\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    embedding_pad = []\n",
    "    embedding_unk = []\n",
    "    word2idx = defaultdict(list)\n",
    "    with open(filename, mode=\"r\", encoding=\"utf-8\") as rf:\n",
    "        lock = True\n",
    "        for i,line in enumerate(rf):\n",
    "            if lock == True:\n",
    "                word2idx[\"<pad>\"] = 0\n",
    "                embedding_pad = [float(0)]*(int(line.split(\" \")[1]))\n",
    "                embeddings.append(embedding_pad)\n",
    "                word2idx[\"<unk>\"] = 1\n",
    "                embedding_unk = np.random.randn(int(line.split(\" \")[1])).tolist()\n",
    "                embeddings.append(embedding_unk)\n",
    "                lock = False\n",
    "                continue\n",
    "            arr = line.split(\" \")\n",
    "            embedding = [float(val) for val in arr[1: ]]\n",
    "            word2idx[arr[0]] = len(word2idx)\n",
    "            embeddings.append(embedding)\n",
    "    return embeddings, word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, word_dict = load_word_dict_ms(embeddingFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6、生成id化的训练集与测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0958b3ab94c0420691c85484f6fdde13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239a7838d7da4d4fb16f1022e35829f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transform_data(word_dict,train, './5resources/train.csv')\n",
    "transform_data(word_dict,dev, './5resources/dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成test数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1、读取数据\n",
    "df_test = load_data(filename_cut_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2、预处理\n",
    "# 2.1清除原始的脏数据\n",
    "clean1_df_test = filter_original(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a028cba7b4bf4dceb2b3e5d3f0d3f927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='word break:', max=37939, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\Anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9205339bbb934687b6d844b0b76dabbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='word break:', max=37939, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\Anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# 2.2去除comment article字段的标点、停用词\n",
    "clean1_df_test[\"comment\"] = clean1_df_test[\"comment\"].progress_apply(cleanPunStop.clean)\n",
    "clean1_df_test[\"article_wb\"] = clean1_df_test[\"article_wb\"].progress_apply(cleanPunStop.clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37939"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean1_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df_test_article = filter_article_length(clean1_df_test)\n",
    "cleaned_df_test_article_comment = filter_comment_length(cleaned_df_test_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35499"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_df_test_article_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625cefe5622843d0be6e0b69ee63f0df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# with open('./simplify/newOrder_article200comment30pkl353198upvote2/'+'353198_word_dict.pkl', 'rb') as f:\n",
    "#     word_dict_test = pickle.load(f)\n",
    "transform_data(word_dict,cleaned_df_test_article_comment, './5resources_jieduan_pretrian_art200com30up10/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
